data_folder : ./processed_dataset
mixed_precision : fp16
log_dir : ./logs
batch_size : 2
gradient_accumulation_steps: 8
mlm_proba : 0.15
num_workers : 4
num_steps : 1000000
log_interval : 100
save_interval : 50000

dataset_params:
  tokenizer : GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct

model_params:
    vocab_size: 178
    hidden_size: 768
    num_attention_heads: 12
    intermediate_size: 2048
    max_position_embeddings: 2048
    num_hidden_layers: 12
    dropout: 0.1